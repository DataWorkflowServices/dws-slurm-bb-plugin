# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json

#
# Copyright 2022 Hewlett Packard Enterprise Development LP
# Other additional copyright holders may be indicated within.
#
# The entirety of this work is licensed under the Apache License,
# Version 2.0 (the "License"); you may not use this file except
# in compliance with the License.
#
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

name: Unit Test
on:
  workflow_call:

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: recursive

      # Required to use the "--output" option in the docker build
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      # The event file needs to be published during PRs so the
      # "Publish Unit Test" workflow can look up the PR number.
      - name: Publish Event File
        uses: actions/upload-artifact@v3
        if: ( github.event_name == 'pull_request' ) || runner.debug
        with:
          name: unit-test-event-file
          path: ${{ github.event_path }}

      # Run the tests and publish the junit report and raw coverage results
      - name: Unit Tests
        run: docker buildx build
            --target testresults
            -t testresults
            --output test_artifacts 
            .

      # Upload junit report. If a test error occurs (e.g. bad Lua syntax) then
      # the raw code coverage results won't be published.
      - name: Publish Test Results
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results
          path: test_artifacts

      # Process the raw code coverage results
      - name: Unit Test Artifacts
        run: docker buildx build
            --target testartifacts
            -t testartifacts
            --output test_artifacts 
            .

      # Build the code coverage summary from the coverage results. These
      # results still need to be uploaded, so the job shouldn't be allowed to
      # fail yet
      - name: Analyze Coverage Results
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: "test_artifacts/coverage.cobertura.xml"
          badge: true
          fail_below_min: false
          format: markdown
          hide_branch_rate: false
          hide_complexity: true
          indicators: true
          output: both
          thresholds: '80 85'

      # Add the coverage summary to the workflow summary. 
      - name: Adding markdown
        run: cat code-coverage-results.md >> $GITHUB_STEP_SUMMARY

      # Include the coverage summary when uploading test artifacts. This makes
      # the summary available when the "Publish Unit Test" workflow creates the
      # code coverage summary PR comment
      - name: Move Coverage Report
        run: mv code-coverage-results.md test_artifacts/

      # Re-upload the test artifacts. This will overwrite the old artifacts,
      # but the only delta should be the processed coverage results
      - name: Publish Coverage Results
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results
          path: test_artifacts

      # This stage of the docke build has a separate check for unit test
      # failures, after the test artifacts have been collected
      - name: Fail on Test Failures
        run: docker buildx build
            --target test
            -t test
            .

      # Re-run the coverage tool, this time failing the build if the coverage
      # is too low.
      - name: Fail on Low Coverage
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: "test_artifacts/coverage.cobertura.xml"
          fail_below_min: true
          format: text
          indicators: false
          output: console
          thresholds: '70 85'
